# Creating Data Products {#dataproducts}

Communicating the results of your analysis are an essential part of the data science workflow. Analysing data in RStudio is fun, but hard to share with anyone who does not understand the language. 

The purpose of data science is to create value from data by creating useful, sound and aesthetic data products. Analysing data is a rewarding activity, but creating value requires you to communicate the results. 

The generic term for the result of a data science project is a 'data product', which can be a:

* Presentation
* Report
* Application
* Infographic, or;
* Anything else that communicates the results of analyses.

This chapter explains how to share the fruits of your labour with colleagues and other interested parties by generating reports that combine text and analysis through [literate programming](https://en.wikipedia.org/wiki/Literate_programming). 

Before we explain how to create reproducible reports, we delve into the data science workflow.

The learning objectives for this chapter are:

* Understand and implement the workflow for data science projects
* Understand and apply the principles of reproducible research
* Apply basic RMarkdown to create a presentation

The data and code for this session are available in the `chapter_07.Rmd` file in the `casestudy1` folder of your RStudio project.

## Data Science Workflow

The workflow for analytical projects starts with defining a problem that needs solving (Figure \@ref(fig:workflow)). The next step involves loading and transforming the data into a format that is suitable for the required analysis. The centre of the data science workflow contains a loop, the so-called data vortex. This vortex consists of three steps: exploration, modelling and reflection. The analyst repeats these steps until the problem is solved or found to be unsolvable.

```{r workflow, fig.cap="Data science workflow.", echo=FALSE, out.width = '60%'}
knitr::include_graphics("images/workflow.png")
```

### Define

The first step of a data science project is to define the problem. This step describes the problem under consideration and the desired future state. The problem definition should not make specific reference to available data or possible methods but be limited to the issue at hand. 

An organisation could seek to optimise production facilities, reduce energy consumption, monitor effectiveness, understand customers, and so on. A concise problem definition is necessary to ensure that a project does not deviate from its original purpose or is cancelled when it becomes apparent that the problem cannot be solved.

The problem definition opens with a description of the current situation and identifies which aspect needs improvement or more profound understanding. The problem statement concludes with a summary of the data product. For example:

> The regulator for water quality has released a new guideline that lowers the maximum value for trihalomethanes (THMs) at the customer tap to 0.20 mg/l. This report assesses the historical performance of the Gormsey water system to evaluate the risk of non-compliance, assuming no operational changes are implemented.

### Prepare

The available data needs to be loaded and wrangled into the required format before any analysis can take place. Anecdotally, this phase of the project could consume up to eighty per cent of the work effort. [Chapter 8](#cleaning) discusses how to automate data cleaning.

Best practice in data science is to describe every field used in the analysis to ensure the context in which the data was created is understood. Including a transparent methodology is where the science comes into data science, and this is where it distinguishes itself from traditional business analysis.

For the problem statement above, we have the Gormsey data from the previous chapters. This data is already ‘tidy‘ and does not need cleaning. [Chapter](#customers) 9 further explains the concept of tidy data. We just need to filter the data to contain only the THM values.

```{r, message=FALSE, echo=FALSE}
library(tidyverse)
gormsey <- read_csv("../casestudy1/gormsey.csv")
gormsey_thm <- filter(gormsey, Measure == "THM")
```

```{r, message=FALSE, eval=FALSE}
library(tidyverse)
gormsey <- read_csv("casestudy1/gormsey.csv")
gormsey_thm <- filter(gormsey, Measure == "THM")
```

### Understand

Once the data is available in a tidy format, the process of understanding the data can commence. The analytical phase consists of a three-stage loop, the data vortex. These three stages are: explore, model and reflect.

The techniques used in this phase depend on the type of problem that is being analysed. Also, each field of endeavour uses different methodological suppositions. Analysing subjective customer data requires a very different approach than the objective reality of a THM test in a water laboratory.

For our example case study, the analysis is a straightforward description of the distribution of the results. We don't have chlorine residuals for the Gormsey data, but if we did, we could, for example, investigate the relationship between chlorine and THM.

#### Explore
The first step when analysing data is to understand the relationship between the data and the reality it describes. Generating descriptive statistics such as averages, ranges, distribution curves and other summaries, provides a quick insight into the data. Relying on numerical analysis alone can, however, deceive because very different sets of data can result in the same values. 

Justin Matejka and George Fitzmaurice from *AutoDesk* demonstrated how very different sets of data could have almost the same [summary statistics](https://doi.org/10.1145/3025453.3025912) (Figure \@ref(fig:datasaurus)). 

Each of these six visualisations shows that these sets of data have very different patterns. When, however, analysing this data without visualising it, the mean values of $x$ and $y$, and their correlations are almost precisely the same for all six subsets. In their paper, they presented an algorithm that generates several patterns with the same summary values, six of which are shown in the illustration.

```{r datasaurus, fig.cap="Six patterns with very similar summary statistics.", echo=FALSE, out.width = '60%'}
knitr::include_graphics("images/datasaurus.png")
```

The data and code to create this visualisation are available in the `casestudy2` folder of your course material.

Another reason visualisations are essential to explore data is to reveal anomalies, such as spikes in time series or outliers. A sudden increase and decrease in physical measurements are often caused by issues with measurement or data transmission instead of actual changes in reality. These spikes need to be removed to ensure the analysis is reliable. Anomalies in social data such as surveys could be subjects that provide the same question to all answers, discussed in the previous chapter.

Detecting and removing outliers and anomalies from the data increases the reliability of the analysis. Not all oddities in a collection of data are necessarily suspicious, and care should be taken before removing data. The reasons for excluding any anomalous data should be documented so that the analysis remains reproducible. 

#### Model

After the analyst has a good grasp of the variables under consideration, the actual analysis can commence. Modelling involves transforming the problem statement into mathematics and code. Every model is bounded by the assumptions contained within it. Statistician George Box is famous for stating:

> "All models of reality are wrong, but some are useful". 

Since data science is not a science in the sense that we are seeking some universal truth, a useful model that can positively influence reality is all we need.

When modelling the data, the original research question always needs to be kept in mind. Exploring and analysing data without a specific purpose can quickly lead to wrong conclusions. Just because two variables correlate does not imply that there is a logical relationship. A clearly defined problem statement and methodology prevent data dredging. 

The full availability of data and the ease of extracting this information makes it easy for anyone to find relationships between different sources of information. This problem is what Drew Conway coined the danger zone in his data science Venn diagram (figure \@ref(fig:conway)). While it is easy to combine data, we should never do this without understanding the mathematical foundations. 

The [Spurious Correlations](http://tylervigen.com/spurious-correlations) website hosts an amusing collection of strong but nonsensical correlations. Did you know that the per-capita cheese consumption in the US correlates strongly with the number of people who die by becoming tangled in their bedsheets?

A good general rule when analysing data is to distrust your method when you can easily confirm your hypothesis. If this is the case, triangulate the results with another method to test your assumptions.

#### Reflect

Before you can communicate the results of an analysis, domain experts need to review the outcomes to ensure they make sense and indeed solve the problem stated in the definition. The reflection phase should, where relevant, also include customers to ensure that the problem statement is being resolved to their satisfaction.

Visualisation is a quick method to establish whether the outcomes make sense by revealing apparent patterns. Another powerful technique to reflect on the results is sensitivity analysis. This technique involves changing some of the assumptions to test the model responds as expected. 
Mathematical models are often complicated, and the relationship between variables is not clearly understood, especially in machine learning applications. Sensitivity analysis helps to understand these relationships by using extreme for specific variables and then observe the effect on the model.

### Communicate

The last, and arguably, the hardest phase of a data science project is to communicate the results to the users. In most cases, the users of a data product are not specialists with a deep understanding of data and mathematics. The difference in skills between the data scientist and the user of their products requires careful communication of the results.

Detailed reports and visualisations need to provide an accurate description of the outcomes, and they need to convince the reader. The most critical aspect of successfully communicating the solution to a problem is to ensure that the consumers of the results understand them and are willing to use the data product to solve their problem. As much as data science is a systematic process, it is also a cultural process that involves managing the internal change in the organisation.

To claim that a report needs to be written with clarity and correct spelling and grammar almost seems redundant. The importance of readable reports implies that the essential language a data scientist needs to master is not Python or R but English, or whatever language you communicate in.

Writing a good data report enhances the reproducibility of the process by describing all the steps in the process. A  report should also help to explain any complex analysis to the user to engender trust in the results.

The topic of writing useful business reports is too broad to do justice within the narrow scope of this course. For those people that need help with their writing, data science can also assist. There are many great online writing tools to support authors not only with spelling but also grammar and idiom. These advanced spelling and grammar checkers use advanced text analysis tools to detect more than spelling mistakes and can help fine-tune a text utilising data science. However, even grammar checking with machine learning is not a perfect replacement for a human being who understands the meaning of the text.
	
## Reproducible Research

RStudio has several options to create shareable outputs with people who don't necessarily understand R code. This section explains how to create reproducible research using R Markdown.

[Chapter six](#ggplot) showed how to use the *ggplot2* package to create aesthetic visualisations and save them to disk in a high resolution with the `ggave()` function. You can then load these images in your report to communicate the results. 

This approach works fine until you need to change some assumptions in your graphs, a new colour scheme or any other change. Every time you change the analysis, you need to edit the report. This workflow is not only inefficient, but it can also lead to error as you might forget to transpose the new results into the report.

The problem with the traditional approach is that the data, the code are separated from the final data product. Reproducible research solves this problem by combining the data and the analysis with the final result.

The most effective method to achieve full reproducibility is to use literate programming. Although many systems exist that at first instance might seem more user-friendly than writing code, point-and-click systems have severe limitations, and the results are often impossible to verify. The central concept of literate programming is that the data, the code and the output are logically linked so that when either the data or the code changes, the output will change as well.

Several methods are available in the R language to ensure analysis is reproducible. The most basic one is adding comments to the code. Comments help a human reader understand the flow of logic. There is a point, however, where your analysis is so complex that you need more comments than code. When this is the case, you need to use more advanced methods. Furthermore, most consumers of data products are not interested in the code and only want to see the results. The next section explains how to use RStudio to create data products in various formats, such as Word, Powerpoint, PDF and HTML.

## R-Markdown

R-Markdown is a method to combine a narrative with the results of the analysis. An R-Markdown file consists, as the name suggests, of R code and Markdown code. You know what R is, so now we need to explore Markdown.

Contemporary software follows the “What You See is What You Get” (WYSIWYG) principle. Graphical interfaces simulate the physical world by making objects on the screen look like pieces of paper and folders on a desk. You point, click and drag documents into folders; documents appear as they would on paper and when done, they go into the rubbish bin. Graphical interfaces are a magic trick that make you believe you are doing physical things. This approach moves people away from understanding how a computer works.

RStudio and other text editors use the “What You See Is What You Mean” (WYSIWYM) principle. As I am writing this book, I don’t see what it will look like in printed form as you would using modern word processors. In RStudio, I only see text, images and some instruction for the computer on what the final product should look like. This approach allows me to focus on writing text instead of worrying about the end product.

The WYSIWYG approach distracts the mind from the content and lures the user into fiddling with style instead of writing text. Office workers around the globe waste a lot of time trying to format or typeset documents. As I am writing this book, it only takes just a couple of keystrokes to convert the text into a fully formatted ebook or web page, ready for distribution.

Many writers don't use WYSIWYG software but produce text directly in a markup language such as LaTeX, HTML or Markdown. The advantage of this approach is that you focus on content instead of design. Anything written in a markup language can be easily exported to almost any format imaginable using templates.

Markdown is, paradoxically, a markup language, known for its simplicity, which is the reason for the play of words. This book is written in RMarkdown and the [bookdown](https://bookdown.org/) package. All the text, images and other resources are available in the `r4h2o-book` folder. The bookdown package processes all the Rmarkdown files and code and produces a book in HTML, PDF and/or EPub format.

Let's put this theory into practice. Go to the *File* menu and create a new R Markdown file. You see a popup menu where you can enter the document title, author name and select the output type. Select *Presentation* and then *PowerPoint* as the output and enter a title related to the problem statement (Figure \@ref(fig:rmdmenu)). When you click OK, RStudio creates a template document that explains the basic principles.

```{r rmdmenu, fig.cap="R Markdown popup menu.", echo=FALSE, out.width = "60%"}
knitr::include_graphics("images/rmarkdown-menu.png")
```

When you click the *Knit* button, RStudio asks you to save the file and generates a document that includes the written content, some of the code, and the output of any R code embedded in the report. When you do this for the first time, you might receive a message that specific packages need to be installed. Follow the prompts to let that happen.

An R-Markdown document consists of four elements:

- Metadata
- Document settings
- Formatted text
- Code and output

The first few lines of the new document are the metadata. This is where you define the title, author name, date and output format. This data is copied from the popup menu, but you can edit it right here between the three dashes.

```
---
title: "Gormsey Trihalomethane Compliance"
author: "Dr Peter Prevos"
date: "15/11/2021"
output: powerpoint_presentation
---
```

The next line sets the overall parameters for the document. These parameters determine how the R code is evaluated and presented. Any R code needs to be embedded in a 'chunk' marked by three backticks and settings between curly braces. You can find the backtick under your escape key.

The line of R code sets the default settings for all following chunks. In this example, `echo = FALSE` means that the output document does not include the code. You can override these settings in the individual code chunks, described below.

````
```{r setup, include=FALSE}`r ''`
knitr::opts_chunk$set(echo = FALSE)
```
````

The third part of the document is text in Markdown format, which looks something like this:

```
## R Markdown

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

## Slide with Bullets

- Bullet 1
- Bullet 2
- Bullet 3
```

The list below shows some of the most common syntax in Markdown:

- Headings: `# H1`, `## H2` etc.
- Bold: `**bold text**`
- Italic:	`*italicized text*`
- Blockquote: 	`> blockquote`
- Ordered List:
    - `1. First item`
    - `2. Second item`
    - `3. Third item`
- Unordered List: 	
    - `- First item`
    - `- Second item`
    - `- Third item`
- Code 	`` `code` ``
- Horizontal Rule: `---`
- Link: 	`[title](https://www.example.com)`
- Image: `![alt text](image.jpg)`

When using PowerPoint as the output format, level 1 or level 2 headings (in absence of any H1) indicate new slides.

The most important part of an R Markdown document are the code chunks that analyse the data and produces the output. The output of these functions will be knitted into the PowerPoint document.

````
## Slide with R Output

```{r cars, echo = TRUE}`r ''`
summary(cars)
```

## Slide with Plot

```{r pressure}`r ''`
plot(pressure)
```
````

You can add additional chunks with the insert button or by pressing `control-alt i`. When you click the button, you notice that RStudio can also process other data science languages such as SQL or Python.

The Rmarkdown package has a lot of options to control how the code is evaluated and the output formatted. You can set them for each chunk, or set them as defaults in the first code chunk. Some examples:

- `echo`: Show or hide the code itself (`TRUE` or `FALSE`)
- `fig.width` and `fig.height`: size of any plots in inches
- `message`: Show or hide messages
- `warning`: Show or hide warnings

Lastly, you can embed the output of an R expression inside a line of text. For example, to write: "A total of 264 THM results appear in the data." To achieve this, you can embed R code within a sentence:

```{r, eval=FALSE}
A total of `r nrow(gormsey_thm)` THM results appear in the data.
```

R evaluates the expression when you knit the document. This way, your numbers are always up to date with the latest data. Make sure you don't forget the lower case letter `r` to indicate that it needs to be evaluated.

When working on a project, it is best first to write the code in a well-commented R script and copy this into a Markdown document after you complete the analysis. You can then add explanatory text to create reproducible research.

> Hit the knit button and review the correspondences between code and output. Change the script and see how it changes the output.

### Formatting tables

The output of data analysis is often expressed in tables. To create neat tables in a report, you should use the `kable()` function of the *knitr*  package. This example below shows how to export a table in an R-Markdown file.

```{R}
library(knitr)
thm_fail <- filter(gormsey, Measure == "THM" & Result > .25)
kable(select(thm_fail, Date, Town, Result),
      caption = "Example outut of the `kable()` function.", 
      digits = 2)
```

The *knitr* library provides functions to knit R and Markdown into the preferred output. The second line filters the Gormsey laboratory data with failed THM tests. The last line converts this data frame into a well-formatted table for Word, HTML or PDF.

### Presenting numbers

The numerical output of R functions often contains far too many decimals. Several functions are available to control the way R presents numbers. Firstly, you can set the default number of digits with the `options(digits = n)` function. 
Standard R is accurate up to about 15 decimals. If you need more decimals, then you need to use specialised packages, such as the *[Rmpfr](https://cran.r-project.org/web/packages/Rmpfr)* package.

The `round()` function defaults to round to an integer or a defined number of decimals. When using a negative number in the `digits` option, the number is rounded to the nearest power of ten. The `floor()` and `ceiling()` functions result in an integer. If you like to constrain the number of total digits, including the integer part, use the `signif()` function.

```{R, echo=TRUE}
a <- sqrt(777)

print(a)

round(a) # Show as integer

round(a, digits = 2) # Round to two digits

round(a, digits = -1) #Rounding to a power of ten

floor(a) # Remove all digits

ceiling(a) # Round up to the nearest integer

signif(a, 5)
```

>**Practice Task**: Try these different rounding and display functions with some random number to explore how they work.

The `kable()` function discussed in the previous section has a built-in rounding function. The `digits` option sets the number of digits for numerical output. If you apply a vector to this option, then you can set separate digit numbers for each column.

More advanced formatting of numbers is available through the `format()` function.  The format function has a lot of options to change the way you display numbers. The most common option is `big.mark = ","`, which adds a comma to separate thousands. The `scientic` option toggles scientific notation on and off.

```{R}
format(2^32, big.mark =",")

format(2^128, big.mark =",")

format(2^128, big.mark =",", scientific = FALSE)
```

>**Practice Task**: Read the help files for `format()` and try some of the other options.

### Export formats

Rstudio can export R-Markdown to many standard formats. A standard file can be 'knitted' to an HTML file for websites, Word, PowerPoint or PDF. To create a PDF, you need to have the LaTeX software installed on your computer. LaTeX is a powerful markup language, often used for publications in the formal and physical sciences.

The export format is listed in the first few lines, the front matter, of the markdown file. The example below shows how to define a title, author name and define the output as a Powerpoint presentation. This example also shows how to relate the output to a template (`r4h2o.potx`). This tool allows you to create data-enabled presentations or reports using your organisation's templates.

```
---
title: "Gormsey Trihalomethane Compliance"
author: "Peter Prevos"
output:
  powerpoint_presentation:
    reference_doc: r4h2o.potx
---
```

Please note the indentation of 2 and 4 spaces at the start of the lines, you need to follow this exactly for RStudio to recognise the data structure.

## Further Study

The `casestudy1`folder contains an R Markdown file with an example using the Gormsey data. This file analyses a problem and presents the analysis as a slide deck.

One of the slides uses a two-column layout. To achieve this in Markdown you need to use the slightly cumbersome format shown below:

```
::::::{.columns}
:::{.column}
Text / code goes here
:::
:::{.column}
Text / code goes here
:::
::::::
```

> Reverse engineer this R Markdown file to practice creating PowerPoint presentations.

This chapter shows that Markdown is a powerful piece of software that allows you to combine code with text. Writing in plain text has many advantages over using text editors. Many journals are now expecting that authors use literate programming to imporve the peer review process.

If you like to learn more, then read the *[R Markdown Definitive Guide](https://bookdown.org/yihui/rmarkdown/)*. This book tells you everything you ever wanted to know about this software, but were afraid to ask.

For more detailed control of the design of tables, use the [Flextable](https://davidgohel.github.io/flextable/) package. This library provides fine-grained control over fonts, colours, sizes and anyting else you like to play with.

You have now completed the first case study. The [next case study](#cleaning) deals with data from a customer survey and analyses their experiences.
