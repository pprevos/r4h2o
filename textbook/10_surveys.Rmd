# Analysing the customer experience {#survey}

```{r include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

The code in the previous two chapters cleaned customer survey data and explored the content of the involvement construct. We will dig deeper into the involvement data to test its reliability and validity. We will write some code to reduce the number of dimensions so we can draw a conclusion about the level of involvement. 

Water professionals often focus on the tangible aspects of water services and measure performance in cubic metres, gallons or kilolitres. The customer experience does not relate to these physical variables as it is a psychological dimension. 

While water utilities tout their services as essential for life, only a small amount of water consumption is required to sustain life. Most water consumption meets other critical needs, such as social belonging and self-esteem. For example, our daily shower is not strictly necessary for health reasons. Anthropologically, the daily shower is a ritual that prepares us for the day. The daily shower is perhaps one of the greatest contributes to the economy because it is also a contemplative moment where we have our great ideas. This qualitative aspect adds additional complexity to how we analyse this data.

This chapter discusses how psychographic surveys relate to consumer psychology and some techniques to analyse this type of data. The learning objectives for this chapter are:

* Asses the reliability and validity of customer surveys
* Create and visualise a correlation matrix
* Use hierarchical clustering to reduce data dimensions

The data is available in the `casestudy2` folder of the [RStudio project](https://github.com/pprevos/r4h2o/).

## The Reliability and Validity of Survey Results

The complexity with processing survey results is to ensure a causal relationship between the mental state, the latent variable and the survey questions (figure \@ref(fig:mental-state)). This strength of this relationship is the validity of the survey. Determining the validity of a survey is a complex matter that is outside the scope of this book. Various methods exist in the literature that address this problem, some of these are:

- *Face validity*: Do the survey questions at face-value relate to the mental state? This is usually a qualitative assessment through a panel of experts.
- *Content validity* – A judgement about whether the survey instrument captures all the relevant components of the latent variable.
- *Construct validity*: Does the construct (the way the latent variable is measured) describe the latent variable (how much variance does the model describe?)

Researchers create these question banks by developing theoretical models of the relationship between mental state and the latent variable and construct lots of survey questions. These surveys are administered and the responses are analysed for validity.

As discussed in the [chapter two](#datascience), data science needs to be valid and reliable. Validity in this sense means that a survey actually measures the psychological construct we seek to understand. Reliability means that the responses have an acceptable level of accuracy and consistency across samples.

The state of mind of the customer is a latent variable, which means that it cannot be directly observed. We can only infer these variables by analysing manifest (observable) variables. The manifest variables in the case of a survey are the answers that respondents provide.

The *Personal Involvement Inventory* is a statistically validated psychological construct. This means that the researchers followed a formal process to assess the validity and reliability of the survey tool. 

Figure \@ref(fig:pii-model) shows the theoretical model for the Personal Involvement Inventory and the ten questions. The ovals are latent the variables, and the squares are the manifest (measured) variables. The manifest variables are the answers to the survey questions. The straight arrows imply causality and the curved arrow indicates a correlation. This diagram thus means that involvement has cognitive and affective dimensions that correlate with each other. These two latent variables cause the respondent to answer each of the five items. The stronger the latent variable, the higher the response on the survey item.


```{r pii-model, fig.cap="Personal Involvement Inventory statistical model.", echo=FALSE, out.width = "30%"}
knitr::include_graphics("images/pii-model.png")
```

This diagram, which is created with factor analysis, shows how we can reduce the ten PII items to one or two variables. If all manifest variables within one construct stringly realte to each other, then we have reason to believe that the survey is reliable. We can also use other techniques, such factor analysis to reduce the ten survey questions to one or two dimensions. Another technique to test the validity of a survey instrument is cluster analysis.

Measuring latent variables is complicated because we need to account for a lot of issues. Firstly, the structure of the questionnaire and the wording of the questions can introduce [response bias](https://en.wikipedia.org/wiki/Response_bias). A bias is a situation where the respondent provides a response that is not causally related to the construct we like to measure. A bias introduces a systematic error in the analysis. Researchers have identified many causes of response bias. We have already seen how the survey managed inattentive customers with a trap question.

## Correlations

The first step to assess the reliability of survey questions is to test the intercorrelation between the items. In the case of the ten involvement questions, if the responses highly correlate with each other, then we can be more justified in reducing the ten responses to one. Following the model in Figure 10.1, we can expect `p01-p05` and `p06-p10` to be highly positively correlated.

The R language provides some functions to calculate correlations. Before we can do this, we need to convert the data structure from long to wide format. In the previous chapter, you saw the `pivot_longer()` function. The tidyr package also provides the `pivot_wider()` function, which achieves the reverse. Alternatively, you can, of course, start again from the raw data.

The [tidyr](https://tidyr.tidyverse.org/) package documentation provides a detailed explanation. Each Tidyverse package has a pdf cheat-sheet that provides a concise overview of the functionality.

```{r, message=FALSE, echo=FALSE}
library(readr)
pii_long <- read_csv("../casestudy2/pii_long.csv")

library(tidyr)
pii_wide <- pivot_wider(pii_long, names_from = Item, values_from = Response)
```

```{r, message=FALSE, eval=FALSE}
library(readr)
pii_long <- read_csv("casestudy2/pii_long.csv")

library(tidyr)
pii_wide <- pivot_wider(pii_long, 
                        names_from = Item, 
                        values_from = Response)
```

>**Practice task**: Run this code and investigate the results. Read the package documentation to help you understand the code.

The `cor()` function calculates the correlation between two vectors. The output of this function is a single number. We find that the correlation between `p01` and `p02` is approximately 0.65. We could do the same action for all combinations, but that would be tedious. 

The correlation function can also analyse a whole data frame at once. In this case, we need to remove the first column because it is not a numerical variable. The output of this code is a matrix of correlations between all combinations of `p01` to `p10`:

```{r}
cor(pii_wide$p01, pii_wide$p02)

c_matrix <- cor(pii_wide[, -1])
round(c_matrix, 2)
```

A matrix is rectangular data, just like a data frame, but there are no variables, only index numbers. Also, in a matrix, all entries need to be of the same data type (number, character, logical), while in a data frame, each column can have a different data type. The R language has several functions to undertake linear algebra with matrix variables.

You access the data in a matrix by its index numbers, e.g. `c_matrix[row, col]`.

The diagonal correlations are logically all 1.00, and all values are repeated above and below the diagonal.

The `cor()` function provides various ways to deal with missing data. Read the help file of this function to learn about the options. In this case, all missing data was removed previously and thus does not need to be accounted for. 

This function provides the common Pearson correlation by default. In some specific cases, you might need to use different methods, which this function can also evaluate. The standard Pearson method for correlations works best with normal distributions. 

>**Practice task**: Read the help file for the correlation function with `help(cor)` and recast the correlation matrix with the Pearson method for correlation.

Answer:

Both methods seem to produce identical results.

```{r, eval=FALSE}
round(cor(pii_wide[, -1], method = "pearson"), 2)
```

We can glance at the output and note that all correlations are positive, which is a first confirmation of the reliability of the PII data. If the questions did not all relate to an underlying latent variable, then the correlation matrix would be less uniform. A uniform correlation matrix suggests that the data possibly describes an underlying phenomenon, which in the case is a consumer’s involvement with tap water.

### Visualising correlation 

Correlations can be visualised with a scatter-plot with each of the variables on the $x$ and $y$-axes. The `geom_point()` geometry in the *ggplot2* package creates scatter-plots. Visualising the data from the survey this way is problematic because we only have responses between 1 and 7 and many points will be plotted on top of each other, so-called overplotting. One of the solutions to this problem is to add jitter to the data. Jitter is a small random amount of variation applied to each data point. The *ggplot2* package uses the jitter geometry (`geom_jitter()`) to implement this technique (Figure \@ref(fig:jitter)). The `width` and `height` variables determine the spread of the points. The `alpha` parameter sets the opacity of the points, with 0 being totally transparent. This way, points that are on top of each other are darker.

```{r jitter, fig.cap="Scatterplot of items p01 and p02.", fig.asp=9/16, out.width="100%"}
library(ggplot2)

ggplot(pii_wide, aes(p01, p02)) + 
    geom_jitter(width = .5, height = .5, alpha = .5) + 
    labs(title = "Scatterplot of items p01 and p02") + 
    theme_bw(base_size = 10)
```

Creating a scatter plot for each permutation in the data would be a lot of work. Several specialised R packages provide functionality to visualise a correlation matrix. The corrplot package provides extensive functionality to visualise correlation matrices. Remember that before you can use this library, you need to install it with `install.packages("corrplot")`.

```{r corrplot, fig.cap="Correlation matrix for PII.", message=FALSE, out.width="100%", fig.asp=9/16}
library(corrplot)

corrplot(c_matrix, type = "lower", diag = FALSE)
```

Figure \@ref(fig:corrplot) shows that the first five items correlate more strongly with each other than with the other five items, and vice versa. This is another indication that the model for PII matches what we see in this survey.

>**Practice task**: Read the corrplot documentation and create different versions of this correlation matrix.

This is only one example of a multivariate correlation plot. Many other packages are available to achieve the same result.

### Statisical significance

The basic R functionality also has a function to test the statistical significance of a correlation. The `cor.test()` function takes two vectors as input and provides the 95% confidence interval. 

> Evaluate this code in the console and review the output.

```{r}
c_test <- cor.test(pii_wide$p01, pii_wide$p02)

c_test
```

The output provides a wealth of statistical information about this correlation. 

The `t` and `df` values relate to the significance statistics. The `p` value tells us that the relationship between these to variables is based on coincidence is very small ($p < 2.2 10^{-16}$). In social science, a value of less than 0.05 is often considered statistical significant. However, you need to be careful in interpreting this outcome because a correlation is only a starting point for analysis.

The output of this cluster analysis is a list, which is another type of R variable. You have already seen scalar variables, vectors, data frames and matrices. A list is the most flexible type of data and is often used to store the results of an analysis. Most complex analytical functions in R provide output in the form of a list.

A list is a set of R variables which can combine scalars, vectors, data frames ad matrices in one indexable structure. The `c_test` variable is a list with nine variables embedded in it. You can view the structure of a list with the `str()` (structure) function.

```{r}
str(c_test)
```

Just like with a data frame, you can access the subsets of a list with the `$` indicator. To display only the $p$-value, for example, use `c_test$p.value`.

### Missing values in correlations

We have removed any missing values from the PII data sets, so all calculations go smoothly.

### Limitations of correlations

Correlations are interesting number, but they are an insufficient metric to draw conclusions about the world. A strong correlation is only an invitation to undertake further research. The old adage 'correlation is not causation' certainly is valid in this case. 

The strong correlation between the survey items does not mean that these responses cause each other. Instead, the correlation indicates that there might be an underlying cause that causes them to correlate. Our hypothesis is that this  cause is the psychological construct of involvement, which is what we set out to measure.

## Cronbach's Alpha

Strong Correlations between surveys items are a good indicator of their reliability and validity, but it is not sufficient evidence.  The most commonly used method to assess the reliability of a survey instrument is Cronbach's Alpha or 'coefficient alpha'. 

The coefficient is a number between 0 and 1 and a value of larger than 0.7 is generally considered acceptable. The higher the coefficient, the higher the reliability.

This coefficient is determined by analysing the covariance between the survey items. Covariance is the degree to which the deviation of a variable from its mean is related to the deviation of another variable from its mean. Covariance is the starting point of linear regression, which is the topic of the next chapter.

Covariance is similar to correlation. However, correlation looks at how two variables interact with each other (strength and direction) instead of their shared variance. Correlation values range from +1 to -1. On the other hand, covariance values can take any value, depending on the absolute values of the variables.

The sample covariance of two variables $X$ ad $Y$ is given by:

$$cov(X, Y) = \frac{\sum(X_i - \bar{X})(Y_i-\bar{Y})}{n-1}$$
Covariance and corrrelations are closely related. The correlation between two variables $X$ and $Y$ is:

$$cor(X, Y) = \frac{cov(X, Y)}{\sigma{X}\sigma{Y}}$$

The code below calculates the covariance and correlation between two items from the survey using these formulas. The `with()` function is a convenient method to not have the repeat the data frame name every time you use a variable. For example: `with(pii_wide, p01 - mean(p01))` gives the same result as `pii_wide$p01 - mean(pii_wide$p01)`.

```{r}
cov_0102 <- with(pii_wide, sum((p01 - mean(p01)) * (p02 - mean(p02)))) / (nrow(pii_wide) - 1)

cov(pii_wide$p01, pii_wide$p02) == cov_0102

cor_0102 <- cov_0102 / (sd(pii_wide$p01) * sd(pii_wide$p02))
```

The `cov()` function calculates the covariance over a set of variables and it works in the same way as the correlation function. The covariance calculated from first principles above can be expressed as `cov(pii$p01, pii$p02)`. To create a covariance matrix use:

```{r}
round(cov(pii_wide[, -1]), 2)
```

Cronbach's Alpha can be determined from the number of survey items, the average covariance between these items and the average variance.

$$ \alpha = \frac{N \bar{c}}{\bar{v} + (N-1) \bar{c}}$$

Here $N$  is equal to the number of items, $\bar{c}$ is the average inter-item covariance among the items and $\bar{v}$ equals the average covariance.

The code below extracts the covariance matrix for the PII data and determines the variables in the formula. The `diag()` function extracts the diagonal from a matrix and the `lower.tri()` function the lower triangle. The result of the `lower.tri()` and `upper.tri()` functions is a matrix with the same size as the input with `TRUE` / `FALSE` indicators for either the diagonal or one of the triangles.

>**Practice task**: Evaluate the `diag()`, `lower.tri()` and `upper.tri()` functions and review the output.

```{r}
pii_cov <- cov(pii_wide[, -1])

N <- ncol(pii_cov)

v <- mean(diag(pii_cov))

c <- mean(pii_cov[lower.tri(pii_cov)])

(N * c) / (v + (N - 1) * c)
```

We see that coefficient Alpha for the PII survey is `r round((N * c) / (v + (N - 1) * c), 2)` and thus sufficient to consider this survey instrument as reliable.

## Hierarchical Clustering to assess validity

The correlation matrix indicates that the responses from customers are reliable as the individual items strongly relate to each other and have a high coeffieicnt alpha. But how do we know that these ten items point to a single latent variable? In other words, are the results of this survey valid; are we actually measuring an underlying psychological phenomenon?

Following the theory of latent variables, the ten measured variables should point to underlying phenomena. Several methods are available to reduce the ten survey dimensions two one or two latent variables. Best practice in psychometric analysis is factor analysis and structural equation modelling. The *[psych package](http://personality-project.org/r/)* provides extensive functionality to undertake such an analysis. Structural equation modelling is a complex topic that is outside the scope of this course.

Another method to reduce the dimensions of a set of data is hierarchical clustering. Clustering is a method to detect patterns in data. Various methods are available to identify clusters, such as _k_-means and hierarchical clustering, which we implement in this case study.

The basic idea of hierarchical clustering is that the algorithm calculates the 'distance' between data points as if they were situated in a geometric space of _n_ dimensions. The algorithm then groups the points that are closest to each other. When the algorithm has found these points, it proceeds to clusters these groups. This process continues until all observations are part of the same cluster.

In machine learning, clustering techniques are often used to reduce the number of variables in a predictive model. Clustering is also a technique that we can use to segment customers by grouping them together.

The basic objective of cluster analysis is to group observations based on their measured features. In this case study, our observations are the ten variables, and the features are the responses provided by customers. We are looking for those items among `p01` to `p10`, that have a similar response pattern. The correlation plot in Figure \@ref(fig:corrplot) already hints to the answer to this problem.

Hierarchical clustering involves five steps:

1. Pre-process the data
2. Scale the data
3. Calculate the distances
4. Cluster the data
5. Review the outcome

### Clustering Analysis Example

The next five sections show how to undertake cluster analysis using a simple two-dimensional example before we analyse the involvement survey data. The simple example contains data from ten hypothetical customers (A--J). 

The first data dimension in the test data is the average annual water consumption and the second dimension is the size of teh land on which the house resides. The data consists of random numbers with a known distribution, generated with the `rnorm()` function. Random number in a computer are not truly random as they are calculated with an algorithm. The `set.seed()` function ensures that you always generate the same random numbers, to promote reproducibility. The code is not further explained, and you can reverse-engineer it at your leisure.

This plot also introduces a new geometry. The `geom_label()` geometry shows a text at a coordinate with a little box around it.

Visualising this data (figure \@ref(fig:cexample)) shows immediately that we should find two clusters.

```{r cexample, fig.cap="Clustering example.", out.width="80%", fig.asp=9/16}
set.seed(1234) 

customers <- tibble(id = LETTERS[1:10],
                    property_size = c(rnorm(5, 500, 100), rnorm(5, 1000, 200)),
                    consumption = c(rnorm(5, 500, 200), rnorm(5, 1000, 10)))

ggplot(customers, aes(property_size, consumption)) +
    geom_label(aes(label = id)) +
    labs(title = "Simulated weekday and weekend consumption",
         subtitle = "Cluster analysis example",
         x = "Property Size", y = "Annual consumption") +
    theme_bw(base_size = 10)
```

>**Practice Task**: Run this code a few times, with and a few times without the `set.seed()` function call and notice the difference.

### Pre-processing

For hierarchical clustering, the columns need to contain the features by which we want to cluster (the observations), and the rows need to hold the variable we want to cluster by.

The data in the example is in the format we want it to be as the rows contain the clustering variable (customers), and the columns (weekday and weekend) are the features by which we seek to cluster.


### Scaling

When the features are not on the same scale, we first need to normalise the data. In our example, the size of the land and water use are totally different measurements that need to be scaled.

The `scale()` function normalises data. The default setting of this function scales each element by subtracting the mean and dividing the result by the standard deviation. The long form of scaling a variable is:

```{r}
with(customers, (consumption - mean(consumption)) / sd(consumption))
```


The input for the scale function in this example are the features of the customers data frame (excluding the first column as this is an identifier).  The output of the `scale()` function is a matrix with normalised observations.

```{r}
customers_scaled <- scale(customers[, -1])
customers_scaled
```

### Distances

Hierarchical clustering requires the distance between observations. Several methods are available to calculate distances, of which two are discussed below. 

```{r dis, fig.cap="Euclidean and Taxi Cab distance", fig.asp=9/16, out.width="80%"}
plot(c(3, 7), c(3, 7), pch = 19, axes = FALSE)
lines(c(3, 7, 7), c(3, 3, 7), lty = 3, col = "red")
lines(c(3, 7), c(3, 7), lty = 2, col = "blue")
legend("topleft", legend = c("Euclidean", "Taxi cab"), col = c("blue", "red"), 
       lwd = 1, lty = c(2, 3))
```

The most common method is to calculate the Euclidean distance, using the famous Pythagoras formula between two variables $p$ and $q$ is:

$$D = \sqrt{(p_1 - q_2)^2 + (p_1 - q_2)^2}$$

For $n$-dimensional data the distance between two variables $p$ and $q$ is:

$$D_{pq} = \sqrt{\sum_{i=1}^{n}(p_{i}-q_{i})^2}$$

Another common method to determine the distance between to point is the so-called taxican or Manhattan distance. This is the distance a taxi would take traversing through a city with a gridded street design, also called [Taxicab Geometry[(https://en.wikipedia.org/wiki/Taxicab_geometry). The taxicab distance is thus the sum of the horizontal and vertical distance:

$$d = |p_1 - p_2| + |q_1 - q_2|$$

The `dist()` function can calculate the distance between the elements in a data frame or matrix. The function can use several methods, with Euclidean distance by default:

```{r}
s <- matrix(c(1, 3, 1, 5), ncol = 2)

s

dist(s)
```

The output of this function is a matrix with the same size as the number of cluster observations. In the example, we are clustering ten customers, so the result is a ten by ten matrix with a 'distance' between each of them. 

```{r}
dist(s, method = "manhattan")
```

However, the matrix only contains the lower triangle. Review the output of the function in the console.

```{r}
customers_dist <- dist(customers_scaled)

round(customers_dist, 2)
```

### Clustering

Now we can find the customer segments. Hierarchical clustering method iterative groups observations until all are clustered into one cluster with all observations. In the example, customers (A, D), (B, C), and (G, I) are the evident first clusters (figure \@ref(fig:cexample)). The algorithm then looks for the next level, which consists of the clusters ((A, D), E), and ((G, I), H). The next step clusters A--E and F--J in two clusters and lastly, all customers are assigned to the supercluster.

```{r}
customer_clusters <- hclust(customers_dist)
```

The output of the `hclust()` function is a summary of the clustering results. When printing it in the console, you get the following result:

```{r, echo=FALSE}
customer_clusters
```

The best way to view the output is to plot it, which gives a dendrogram (tree diagram). The code below uses the base plotting functionality in R and not the *ggplot2* version we have used previously. 

The plot function recognises the input as the result of hierarchical clustering and will visualise it as a tree. The `main` and `sub` options provide the title and subtitle to the plot. The `labels` option adds the names to the cluster numbers (figure \@ref(fig:dendro1)).

```{r dendro1, fig.cap="Dendrogram of the example data.", out.width="80%", fig.asp=9/16}
plot(customer_clusters,
     main = "Clustering Example",
     sub = "Simulated data",
     labels = customers$id)
```

You can view the clusters at each level of the analysis, working your way up to one supercluster. The vertical distance in the graph relates to the distance matrix. The longer the line, the less related the customers are. Visually, both figures \@ref(fig:dendro1) and figure \@ref(fig:segments) suggest that we should have two clusters.

You can extract more information from the clusters with the `cutree()` function. This function allows you to cut the tree at a certain level. The output is a vector of the cluster number that each customer belongs to. At the highest level (`k = 1`), all customers form part of the same cluster. At the lowest level (`k = 10`), all customers are individuals.

Extracting two clusters, we can assign these variables as segments to our customer table and visualise the data. Note the `fill = factor(segment)`. This option assigns a fill colour to the label. The factor function is needed to force R to assign qualitative colours instead of a variable range.

>**Practice task**: Evaluate this function without the `factor()` function to understand the difference.

```{r segments, fig.cap="Clustered customer segments.", out.width="80%", fig.asp=9/16}
customers$segment <- cutree(customer_clusters, k = 2)

ggplot(customers, aes(property_size, consumption, fill = factor(segment))) +
    geom_label(aes(label = id)) +
    scale_fill_manual(values = c("dodgerblue", "lightgrey"), name = "Segment") +
    labs(title = "Simulated weekday and weekend consumption",
         subtitle = "Cluster analysis example",
         x = "Property Size", y = "Annual consumption") +
    theme_bw(base_size = 10)
```

Two clusters are the obvious answer for this problem, but in reality, the boundary between clusters is not always this clear. Interpreting the results and selecting the ideal number of clusters is a combination of scientific insight and statistical analysis.

### Interpreting Cluster Analyses

Clusters can be intuitive but can also be easily misinterpreted. You can analyse any dataset, and you will find clusters, but that does not imply that these clusters are meaningful and significantly distinct from other solutions. Some statistical techniques exist to assess how well the chosen model fits the data, but there is no single objective criterion to determine the ideal number of clusters. 

How do we know the ideal number of clusters? The diagram in Figure \@ref(fig:segments) helps to visually review the number of clusters. The customer data is intuitively best fitted with two clusters, with one uncertain outlier. We can only see this clearly because there are only two dimensions. When the number of dimensions exceeds four, visualising the data this way becomes almost impossible.

The dendrogram in figure \@ref(fig:dendro1) provides a better overview. The vertical line between one and two clusters in the dendrogram is the longest, which means the distance between the two clusters is larger than the distance between any other cluster. 

Another method to visualise the clustering solution is a scree plot, shown in figure \@ref(fig:scree). This plot visualises the distances between each of the cluster solutions so it is easy to see the largest jump in distance. the `height` variable in the results list stores the hieght for each cluster. The order needs to be reversed to with the `rev()` function to create a typical scree plot.

You are looking for the point where the attached lines make the smallest angle, which in this case is number two, confirming the two-factor solution.

```{r scree, fig.cap="Scree plot of the customer clusters.", out.width="60%", fig.asp=9/16}
plot(rev(customer_clusters$height), type = "b")
```

These visual statistical methods often don't provide a conclusive answer. There is no method to determine the best number of clusters with absolute certainty.

The main criterion for the result of cluster analysis is that it makes sense. We strive for parsimony, which means that we want to have the lowest possible number of clusters. But having too few clusters also does not help us much in explaining the structure of the data. The main criterion is whether the cluster model explains the variability of reality.

The two-cluster solution is also suitable because it is reasonably logical that residential customers use water in the weekend, while large commercial customers might use much less water, which confirms the two-cluster solution.

## Clustering the Involvement Data

The data from the involvement survey needs to be transformed again to make it suitable for hierarchical clustering. The observations of the PII survey are the individual customers (the rows), and the features are the ten items (the columns). In this case, our interest goes to the items themselves. We thus need to rotate or transpose the data frame. We can do this with the transpose `t()` function. This function rotates rows to columns and vice versa. The code below rotates the wide survey data, excluding the `id` parameter.

Use `rect.hclust(pii_clusters, k = 2, border = 2:3)` to draw a rectangle around the two-cluster (`k = 2`) solution. The `border = 2:3` defines the colour numbers of the two rectangles.

```{r pii-clust, fig.cap="Clustered customer segments.", out.width="80%", fig.asp=9/16}
pii_clust <- t(pii_wide[, -1]) %>% 
  scale() %>% 
  dist() %>% 
  hclust()

plot(pii_clust,
     main = "Personal Involvement Index",
     sub = "Survey Items", cex = 1.2)
rect.hclust(pii_clust, k = 2, border = 2:3)
```
```{r, echo=FALSE, include=FALSE}
png("images/case-study-02.png", width = 800, height = 450)
plot(pii_clust,
     type = "rectangle",
     main = NULL, 
     sub = "Survey Items", cex = 1.2)
rect.hclust(pii_clust, k = 2, border = "red")
dev.off()
```

We can see that the largest trunk in the dendrogram is with two clusters. We can safely choose this solution because the survey was designed as a two-dimensional construct. We can also see that the cluster analysis confirms the correlation matrix (figure \@ref(fig:corrplot)). The first five and the last five items are closest related to each other. 

This analysis means that we can reasonably sure that each of these five items measures the same underlying latent variable, whcih is confirmed by the scree plot.

```{r scree-pii, fig.cap="Scree plot of the PII clusters.", out.width="80%", fig.asp=9/16}
plot(rev(pii_clust$height), type = "b")
```

## Reviewing the Personal Involvement Index

Now that we have shown that the first five and the last five questions cluster into one latent variable, we can review the level of involvement with tap water reported by the customers. 

The easiest way to do this is by adding the scores for the questions for each dimension and add a total score.

The code below groups the data by survey id (by customer) and calculates the new scores with the `mutate()` function. This function creates a new variable in the data frame. You can add more than one new variable within one function all, as shown below. 

After we have these three variables, we can pivot the data around these values and ditch the individual responses.

```{r, message=FALSE}
library(dplyr)

pii <- pii_wide %>%
    group_by(id) %>%
    summarise(Cognitive = p01 + p02 + p03 + p04 + p05,
              Affective = p06 + p07 + p08 + p09 + p10,
              Involvement = Cognitive + Affective) %>%
    pivot_longer(Cognitive:Involvement, 
                 names_to = "Dimension",
                 values_to = "Score")
```

The next code snippet visualises the data with the histogram geometry. The `bins = 30` option defines the number of columns of the histogram. The `scales = "free_x"` option means that for each facet, a different $x$-scale can be used (figure \@ref(fig:pii-plot)).

```{r pii-plot, fig.cap="Personal Involvement Index for tap water in Gormsey.", fig.asp=9/16}
ggplot(pii, aes(Score)) + 
    geom_histogram(bins = 30) + 
    facet_wrap(~Dimension, scales = "free_x") + 
    labs(title = "Personal Involvement Index",
         subtitle = "Tap Water Customers in Gormsey") + 
    theme_bw(base_size = 10)
```

These results are intriguing as the level of cognitive involvement is much higher than affective involvement. Customers see water more as a necessity than as something they have a relationship with. 

The level of affective involvement is, however, quite high compared to other commodities. This score is perhaps an expression of the types of benefits that we obtain from using tap water. 

The two involvement dimensions have a different distribution. While affective involvement is more or less a normal distribution, cognitive involvement is highly positively skewed.

>**Practice Task**: How would you interpret these scores? How do you explain the significant spike at the highest level of involvement?

## Quiz

The sixth quiz asks some questions about correlations and cluster analysis.

The following questions test your comprehension of some of the theory and functionality explained in this chapter. Test your answer by evaluating the code in the console.

Load the cleaned customer survey data you created in chapter 8. 

### Question 1

What is the correlation between the level of self-reported hardship (`hardship`) and the frequency at which they contact  (`contact`) their utility? 

Remember to manage the missing variables with the `use` option in the correlation function. Check the help file for the correlation function to select the correct option.

### Question 2

What is the likelihood that the relationship between these two variables is coincidental?

### Question 3

Isolate the variables that start with the letter `t` or `f` from the customer data. This data relates to questions about service quality. Transform the data and undertake a hierarchical cluster analysis and review the dendrogram.

How many latent variables would you assign these 18 variables to?

### Question 4

The variables starting with the letter `t` measure technical quality. Determine the total score for `t01` to `t05` for each respondent. What is the mean value of this latent construct?

Use the `mutate()` function to add the scores for each respondent.

That is it for the sixth quiz. If you get stuck, you can find the answers in the Appendix.

## Further study

Accurate measurement of psychological constructs is a complex topic that goes beyond the scope of this course. Please note that the examples in this chapter do not constitute a thorough analysis of latent constructs. Correlations and cluster analysis are great for exploration. Structural equation modelling is best practice in psychographic analysis. If you are interested in the statistical intricacies of measuring the customer experience, then read *Scale Development: Theory and Applications* by Robert Devils (2011). 

### Other techniques

Hiereachical clustering is not the only technique that is available to cluster data. Another popular algorithm is *k*-means, which performs better when you analyse large sets of data.

This article by George Serif provides a comprehensive overview of the various [methods to cluster data](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68) and their differences.

The [next chapter](#reporting) invites you to further analyse the information in the customer survey data and create a report with RMarkdown.
